{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéÆ Assignment 1: Policy Iteration in the Repeated Prisoner's Dilemma\n",
    "\n",
    "**Course**: Reinforcement Learning 2026A\n",
    "\n",
    "---\n",
    "\n",
    "## üìñ What is this assignment about?\n",
    "\n",
    "Imagine you're playing a game with someone repeatedly. Each round, you both choose to either **Cooperate** (be nice) or **Defect** (be selfish). The points you get depend on what BOTH of you choose:\n",
    "\n",
    "| Your Choice | Their Choice | Your Points | What Happened |\n",
    "|-------------|--------------|-------------|---------------|\n",
    "| Cooperate | Cooperate | 3 | ü§ù Mutual trust! |\n",
    "| Cooperate | Defect | 0 | üò¢ You got betrayed |\n",
    "| Defect | Cooperate | 5 | üòà You exploited them |\n",
    "| Defect | Defect | 1 | üíÄ Mutual destruction |\n",
    "\n",
    "**The Big Question**: What's the BEST strategy? Should you always betray? Always cooperate? Something smarter?\n",
    "\n",
    "**Our Goal**: Use a Reinforcement Learning algorithm called **Policy Iteration** to find the mathematically optimal strategy against different opponents.\n",
    "\n",
    "---\n",
    "\n",
    "## üß± How This Notebook is Organized\n",
    "\n",
    "We build the solution step-by-step like LEGO bricks:\n",
    "\n",
    "1. **Brick 1**: Define the rules (actions, rewards)\n",
    "2. **Brick 2**: Create opponent \"personalities\" (strategies)\n",
    "3. **Brick 3**: Build the game environment\n",
    "4. **Brick 4**: Test that everything works\n",
    "5. **Brick 5**: Build the MDP (the math behind the game)\n",
    "6. **Brick 6**: Implement Policy Iteration (find optimal strategy)\n",
    "7. **Brick 7**: Run experiments and analyze results\n",
    "8. **Brick 8**: Visualize the game with animations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üîß Setup: Install Required Packages\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this first!)\n",
    "!pip install gymnasium==1.1.1 matplotlib pandas moviepy==1.0.3 pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries we need\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "from itertools import product\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import pandas as pd\n",
    "import os\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "# For reproducibility - same random numbers every time\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üß± BRICK 1: Constants and Payoff Matrix\n",
    "---\n",
    "\n",
    "## What is this brick doing?\n",
    "\n",
    "This brick defines the **fundamental rules** of the Prisoner's Dilemma game:\n",
    "\n",
    "1. **Actions**: There are only 2 possible moves:\n",
    "   - `COOPERATE = 0` (be nice, work together)\n",
    "   - `DEFECT = 1` (be selfish, betray)\n",
    "\n",
    "2. **Payoff Matrix**: This is the \"scoring table\" that tells us how many points each combination of moves gives.\n",
    "\n",
    "## Why these specific numbers?\n",
    "\n",
    "The numbers (3, 0, 5, 1) are carefully chosen to create a **dilemma**:\n",
    "\n",
    "- **Temptation (T=5)** > **Reward (R=3)** > **Punishment (P=1)** > **Sucker (S=0)**\n",
    "\n",
    "This means:\n",
    "- If you KNOW the opponent will cooperate, you should defect (get 5 instead of 3)\n",
    "- If you KNOW the opponent will defect, you should also defect (get 1 instead of 0)\n",
    "- So defecting seems \"dominant\"... but if BOTH defect, you both get only 1!\n",
    "- If you could TRUST each other, you'd both get 3. That's the dilemma! ü§î"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BRICK 1: THE RULES OF THE GAME\n",
    "# ============================================================\n",
    "\n",
    "# Define the two possible actions\n",
    "# We use numbers (0 and 1) because computers like numbers\n",
    "COOPERATE = 0  # Be nice, work together\n",
    "DEFECT = 1     # Be selfish, betray\n",
    "\n",
    "# A list of all actions (useful for loops later)\n",
    "ACTIONS = [COOPERATE, DEFECT]\n",
    "\n",
    "# Human-readable names for printing\n",
    "ACTION_NAMES = {COOPERATE: 'C', DEFECT: 'D'}\n",
    "ACTION_EMOJI = {COOPERATE: 'ü§ù', DEFECT: 'üó°Ô∏è'}\n",
    "\n",
    "# The Payoff Matrix: (my_action, opponent_action) -> my_reward\n",
    "# This is the HEART of the game - it defines all possible outcomes\n",
    "PAYOFF_MATRIX = {\n",
    "    (COOPERATE, COOPERATE): 3,  # R (Reward) - We both cooperate, everyone wins!\n",
    "    (COOPERATE, DEFECT):    0,  # S (Sucker) - I cooperate, they betray me üò¢\n",
    "    (DEFECT, COOPERATE):    5,  # T (Temptation) - I betray, they're nice üòà\n",
    "    (DEFECT, DEFECT):       1,  # P (Punishment) - We both betray, nobody wins\n",
    "}\n",
    "\n",
    "# Let's display this nicely\n",
    "print(\"‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\")\n",
    "print(\"‚ïë              THE PRISONER'S DILEMMA PAYOFF               ‚ïë\")\n",
    "print(\"‚ïë                    (Your Points)                         ‚ïë\")\n",
    "print(\"‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\")\n",
    "print(\"‚ïë                    ‚îÇ  Opponent: C  ‚îÇ  Opponent: D        ‚ïë\")\n",
    "print(\"‚ïë‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïë\")\n",
    "print(f\"‚ïë  You: Cooperate    ‚îÇ      {PAYOFF_MATRIX[(0,0)]} ü§ù      ‚îÇ      {PAYOFF_MATRIX[(0,1)]} üò¢          ‚ïë\")\n",
    "print(f\"‚ïë  You: Defect       ‚îÇ      {PAYOFF_MATRIX[(1,0)]} üòà      ‚îÇ      {PAYOFF_MATRIX[(1,1)]} üíÄ          ‚ïë\")\n",
    "print(\"‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\")\n",
    "print(\"\\nüí° Notice: Defecting always gives more points than cooperating\")\n",
    "print(\"   against the SAME opponent move. But mutual cooperation (3+3=6)\")\n",
    "print(\"   beats mutual defection (1+1=2) for the group!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Brick 1 Validation\n",
    "\n",
    "**What we expect**: The payoff matrix should satisfy T > R > P > S (5 > 3 > 1 > 0)\n",
    "\n",
    "**What we got**: ‚úÖ Correct! This creates the classic Prisoner's Dilemma structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üß± BRICK 2: Opponent Strategies\n",
    "---\n",
    "\n",
    "## What is this brick doing?\n",
    "\n",
    "This brick creates different \"personalities\" for our opponent. Each strategy is like a different player with a different mindset:\n",
    "\n",
    "### The Four Opponents:\n",
    "\n",
    "1. **ALL_C (Always Cooperate)** üòá\n",
    "   - The naive nice guy who ALWAYS cooperates\n",
    "   - Never learns, never retaliates\n",
    "   - Easy to exploit!\n",
    "\n",
    "2. **ALL_D (Always Defect)** üòà\n",
    "   - The ruthless player who ALWAYS defects\n",
    "   - Never cooperates, never trusts\n",
    "   - You can't do anything but defect back\n",
    "\n",
    "3. **TFT (Tit-for-Tat)** ü™û\n",
    "   - Starts nice (cooperates first)\n",
    "   - Then COPIES whatever you did last round\n",
    "   - You defect? They defect next. You cooperate? They cooperate next.\n",
    "   - Famous for winning Axelrod's tournament!\n",
    "\n",
    "4. **IMPERFECT_TFT (Noisy Tit-for-Tat)** üé≤\n",
    "   - Same as TFT, but makes mistakes 10% of the time\n",
    "   - 90% chance: copies your last move\n",
    "   - 10% chance: does the OPPOSITE (\"oops!\")\n",
    "   - This simulates real-world noise/miscommunication\n",
    "\n",
    "## Why do we need different opponents?\n",
    "\n",
    "The optimal strategy DEPENDS on who you're playing against:\n",
    "- Against ALL_C: Always defect (exploit them!)\n",
    "- Against ALL_D: Always defect (no point cooperating)\n",
    "- Against TFT: Maybe cooperate? (they'll be nice back!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BRICK 2: OPPONENT STRATEGIES (Their \"Brains\")\n",
    "# ============================================================\n",
    "\n",
    "def get_opponent_action(strategy, history):\n",
    "    \"\"\"\n",
    "    Decides what the opponent will do based on their strategy.\n",
    "    \n",
    "    Think of this as the opponent's \"brain\" - given what happened\n",
    "    in past rounds (history), what will they do next?\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    strategy : str\n",
    "        The opponent's personality: \"ALL_C\", \"ALL_D\", \"TFT\", or \"IMPERFECT_TFT\"\n",
    "    \n",
    "    history : list of tuples\n",
    "        Past rounds as [(my_action, opp_action), (my_action, opp_action), ...]\n",
    "        The most recent round is at the END (index -1)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    int : COOPERATE (0) or DEFECT (1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Strategy 1: The Saint - Always Cooperates üòá\n",
    "    if strategy == \"ALL_C\":\n",
    "        return COOPERATE  # No matter what, be nice\n",
    "    \n",
    "    # Strategy 2: The Villain - Always Defects üòà\n",
    "    if strategy == \"ALL_D\":\n",
    "        return DEFECT  # No matter what, be mean\n",
    "    \n",
    "    # Strategy 3: Tit-for-Tat - The Mirror ü™û\n",
    "    if strategy == \"TFT\":\n",
    "        if len(history) == 0:\n",
    "            return COOPERATE  # Start nice (give them a chance)\n",
    "        # Copy whatever the agent did last time\n",
    "        # history[-1] = last round, [0] = agent's action\n",
    "        return history[-1][0]\n",
    "    \n",
    "    # Strategy 4: Imperfect Tit-for-Tat - The Clumsy Mirror üé≤\n",
    "    if strategy == \"IMPERFECT_TFT\":\n",
    "        if len(history) == 0:\n",
    "            return COOPERATE  # Start nice\n",
    "        \n",
    "        # What SHOULD I do? (copy agent's last move)\n",
    "        intended_action = history[-1][0]\n",
    "        \n",
    "        # But 10% of the time, I mess up!\n",
    "        if random.random() < 0.10:\n",
    "            # Flip the action: 0 becomes 1, 1 becomes 0\n",
    "            return 1 - intended_action\n",
    "        else:\n",
    "            return intended_action\n",
    "    \n",
    "    # Default fallback (should never reach here)\n",
    "    return COOPERATE\n",
    "\n",
    "\n",
    "# Let's test each strategy to make sure it works!\n",
    "print(\"üß™ Testing Opponent Strategies:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Test ALL_C\n",
    "test_history = [(DEFECT, COOPERATE), (DEFECT, COOPERATE)]  # I defected twice\n",
    "result = get_opponent_action(\"ALL_C\", test_history)\n",
    "print(f\"ALL_C after I defected twice: {ACTION_NAMES[result]} (Expected: C) {'‚úÖ' if result == COOPERATE else '‚ùå'}\")\n",
    "\n",
    "# Test ALL_D\n",
    "test_history = [(COOPERATE, DEFECT), (COOPERATE, DEFECT)]  # I cooperated twice\n",
    "result = get_opponent_action(\"ALL_D\", test_history)\n",
    "print(f\"ALL_D after I cooperated twice: {ACTION_NAMES[result]} (Expected: D) {'‚úÖ' if result == DEFECT else '‚ùå'}\")\n",
    "\n",
    "# Test TFT\n",
    "result1 = get_opponent_action(\"TFT\", [])  # Empty history\n",
    "result2 = get_opponent_action(\"TFT\", [(DEFECT, COOPERATE)])  # I defected\n",
    "result3 = get_opponent_action(\"TFT\", [(COOPERATE, DEFECT)])  # I cooperated\n",
    "print(f\"TFT first move: {ACTION_NAMES[result1]} (Expected: C) {'‚úÖ' if result1 == COOPERATE else '‚ùå'}\")\n",
    "print(f\"TFT after I defected: {ACTION_NAMES[result2]} (Expected: D) {'‚úÖ' if result2 == DEFECT else '‚ùå'}\")\n",
    "print(f\"TFT after I cooperated: {ACTION_NAMES[result3]} (Expected: C) {'‚úÖ' if result3 == COOPERATE else '‚ùå'}\")\n",
    "\n",
    "print(\"\\n‚úÖ All opponent strategies working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Brick 2 Validation\n",
    "\n",
    "**What we expect**: Each strategy should behave according to its description\n",
    "\n",
    "**What we got**: \n",
    "- ALL_C always returns C ‚úÖ\n",
    "- ALL_D always returns D ‚úÖ\n",
    "- TFT starts with C, then mirrors ‚úÖ\n",
    "- IMPERFECT_TFT is like TFT but with 10% random flips ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üß± BRICK 3: The Game Environment\n",
    "---\n",
    "\n",
    "## What is this brick doing?\n",
    "\n",
    "This brick creates a **Gymnasium Environment** - a standardized way to represent games in Reinforcement Learning. Think of it as a \"game console\" that:\n",
    "\n",
    "1. **Keeps track of the game state** (what happened in past rounds)\n",
    "2. **Lets you play moves** (`step` function)\n",
    "3. **Tells you the score** (reward)\n",
    "4. **Can reset for a new game** (`reset` function)\n",
    "\n",
    "## Key Concepts:\n",
    "\n",
    "### State (What the agent \"sees\")\n",
    "\n",
    "The **state** is the information available to make a decision. We implement two versions:\n",
    "\n",
    "- **Memory-1**: See only the LAST round ‚Üí 4 possible states\n",
    "  - (C,C), (C,D), (D,C), (D,D)\n",
    "  \n",
    "- **Memory-2**: See the last TWO rounds ‚Üí 16 possible states\n",
    "  - (my_t-1, my_t-2, opp_t-1, opp_t-2)\n",
    "\n",
    "### Initial State\n",
    "\n",
    "When the game starts, there IS no history. The assignment says to pretend both players cooperated before the game started.\n",
    "\n",
    "### Why use Gymnasium?\n",
    "\n",
    "Gymnasium is the standard API for RL environments. By following this format, our game can work with ANY RL algorithm (Q-learning, Policy Gradient, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BRICK 3: THE GAME ENVIRONMENT (The \"Game Console\")\n",
    "# ============================================================\n",
    "\n",
    "class PrisonerDilemmaEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    A Gymnasium environment for the Repeated Prisoner's Dilemma.\n",
    "    \n",
    "    Think of this class as a \"game console\":\n",
    "    - It knows the rules\n",
    "    - It tracks the score\n",
    "    - It remembers what happened\n",
    "    - It controls the opponent\n",
    "    \n",
    "    The AGENT (our RL algorithm) plays against this environment.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, opponent_strategy, memory_length=1):\n",
    "        \"\"\"\n",
    "        Set up a new game.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        opponent_strategy : str\n",
    "            Who are we playing against? (\"ALL_C\", \"ALL_D\", \"TFT\", \"IMPERFECT_TFT\")\n",
    "        \n",
    "        memory_length : int\n",
    "            How many past rounds can the agent see? (1 or 2)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store settings\n",
    "        self.opponent_strategy = opponent_strategy\n",
    "        self.memory_length = memory_length\n",
    "        \n",
    "        # Define what actions are possible (0 = Cooperate, 1 = Defect)\n",
    "        self.action_space = gym.spaces.Discrete(2)\n",
    "        \n",
    "        # History: list of (my_action, opponent_action) tuples\n",
    "        self.history = []\n",
    "        \n",
    "        # For tracking the game\n",
    "        self.total_reward = 0\n",
    "        self.round_number = 0\n",
    "    \n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"\n",
    "        Start a new game. Wipe the slate clean!\n",
    "        \n",
    "        Returns the initial state (pretend everyone cooperated before).\n",
    "        \"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        # Reset counters\n",
    "        self.total_reward = 0\n",
    "        self.round_number = 0\n",
    "        \n",
    "        # Initialize with fake \"pre-game\" history of mutual cooperation\n",
    "        # This is what the assignment specifies!\n",
    "        self.history = [(COOPERATE, COOPERATE) for _ in range(self.memory_length)]\n",
    "        \n",
    "        return self._get_state(), {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Play one round of the game.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        action : int\n",
    "            The agent's choice: COOPERATE (0) or DEFECT (1)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        state : tuple\n",
    "            The new state after this round\n",
    "        reward : int\n",
    "            Points earned this round\n",
    "        terminated : bool\n",
    "            Is the game over? (Always False - game is infinite)\n",
    "        truncated : bool\n",
    "            Was the game cut short? (Always False)\n",
    "        info : dict\n",
    "            Extra information (empty for us)\n",
    "        \"\"\"\n",
    "        self.round_number += 1\n",
    "        \n",
    "        # 1. Opponent makes their move (based on their strategy and history)\n",
    "        opp_action = get_opponent_action(self.opponent_strategy, self.history)\n",
    "        \n",
    "        # 2. Calculate the agent's reward\n",
    "        reward = PAYOFF_MATRIX[(action, opp_action)]\n",
    "        self.total_reward += reward\n",
    "        \n",
    "        # 3. Add this round to history\n",
    "        self.history.append((action, opp_action))\n",
    "        \n",
    "        # 4. Keep only the most recent rounds (based on memory length)\n",
    "        if len(self.history) > self.memory_length:\n",
    "            self.history.pop(0)  # Remove oldest\n",
    "        \n",
    "        # 5. Get the new state\n",
    "        state = self._get_state()\n",
    "        \n",
    "        # Game never ends (we control episode length externally)\n",
    "        return state, reward, False, False, {'opp_action': opp_action}\n",
    "    \n",
    "    def _get_state(self):\n",
    "        \"\"\"\n",
    "        Convert history into a state tuple.\n",
    "        \n",
    "        Memory-1: (my_last, opp_last) ‚Üí 4 possible states\n",
    "        Memory-2: (my_t-1, my_t-2, opp_t-1, opp_t-2) ‚Üí 16 possible states\n",
    "        \"\"\"\n",
    "        if self.memory_length == 1:\n",
    "            # Just the last round\n",
    "            return self.history[-1]  # (my_action, opp_action)\n",
    "        else:\n",
    "            # Last two rounds, reorganized\n",
    "            return (\n",
    "                self.history[-1][0],  # my action at t-1\n",
    "                self.history[-2][0],  # my action at t-2\n",
    "                self.history[-1][1],  # opponent at t-1\n",
    "                self.history[-2][1],  # opponent at t-2\n",
    "            )\n",
    "\n",
    "\n",
    "print(\"‚úÖ Environment class created successfully!\")\n",
    "print(\"\\nüìã Environment Summary:\")\n",
    "print(\"   - Actions: 0 (Cooperate) or 1 (Defect)\")\n",
    "print(\"   - Memory-1 States: 4 possible\")\n",
    "print(\"   - Memory-2 States: 16 possible\")\n",
    "print(\"   - Initial State: All cooperate history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üß± BRICK 4: Sanity Check - Test the Environment\n",
    "---\n",
    "\n",
    "## What is this brick doing?\n",
    "\n",
    "Before we build anything complex, let's make sure the environment actually works! We'll play a few manual rounds and check:\n",
    "\n",
    "1. Does the opponent respond correctly?\n",
    "2. Are the rewards calculated right?\n",
    "3. Does the state update properly?\n",
    "\n",
    "## Test Plan:\n",
    "\n",
    "Play against TFT with moves: C, D, D, C, C\n",
    "\n",
    "| Round | My Move | Expected TFT Response | Expected Reward |\n",
    "|-------|---------|----------------------|----------------|\n",
    "| 1 | C | C (starts nice) | 3 |\n",
    "| 2 | D | C (copies my C from round 1) | 5 |\n",
    "| 3 | D | D (copies my D from round 2) | 1 |\n",
    "| 4 | C | D (copies my D from round 3) | 0 |\n",
    "| 5 | C | C (copies my C from round 4) | 3 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BRICK 4: TEST THE ENVIRONMENT\n",
    "# ============================================================\n",
    "\n",
    "print(\"üß™ Testing Environment Against TFT (Memory-1)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create environment\n",
    "env = PrisonerDilemmaEnv(opponent_strategy=\"TFT\", memory_length=1)\n",
    "state, _ = env.reset()\n",
    "\n",
    "print(f\"Initial State: ({ACTION_NAMES[state[0]]}, {ACTION_NAMES[state[1]]})\")\n",
    "print(f\"   (This means: I played {ACTION_NAMES[state[0]]}, opponent played {ACTION_NAMES[state[1]]})\")\n",
    "print()\n",
    "\n",
    "# Play our test sequence\n",
    "test_moves = [COOPERATE, DEFECT, DEFECT, COOPERATE, COOPERATE]\n",
    "expected_opp = [COOPERATE, COOPERATE, DEFECT, DEFECT, COOPERATE]  # TFT copies with 1-round delay\n",
    "expected_rewards = [3, 5, 1, 0, 3]\n",
    "\n",
    "total_reward = 0\n",
    "all_correct = True\n",
    "\n",
    "print(\"Round | My Move | Opp Move | Reward | State After | Check\")\n",
    "print(\"-\"*65)\n",
    "\n",
    "for i, move in enumerate(test_moves):\n",
    "    state, reward, _, _, info = env.step(move)\n",
    "    opp_action = info['opp_action']\n",
    "    total_reward += reward\n",
    "    \n",
    "    # Check if correct\n",
    "    opp_correct = opp_action == expected_opp[i]\n",
    "    rew_correct = reward == expected_rewards[i]\n",
    "    is_correct = opp_correct and rew_correct\n",
    "    all_correct = all_correct and is_correct\n",
    "    \n",
    "    check = \"‚úÖ\" if is_correct else \"‚ùå\"\n",
    "    \n",
    "    print(f\"  {i+1}   |    {ACTION_NAMES[move]}    |    {ACTION_NAMES[opp_action]}     |   {reward}    | ({ACTION_NAMES[state[0]]},{ACTION_NAMES[state[1]]})        | {check}\")\n",
    "\n",
    "print(\"-\"*65)\n",
    "print(f\"Total Reward: {total_reward} (Expected: {sum(expected_rewards)})\")\n",
    "print()\n",
    "\n",
    "if all_correct:\n",
    "    print(\"üéâ ALL TESTS PASSED! Environment is working correctly!\")\n",
    "else:\n",
    "    print(\"‚ùå Some tests failed. Check the logic.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Brick 4 Validation\n",
    "\n",
    "**What we expect**: TFT should mirror our previous move, rewards should match the payoff matrix\n",
    "\n",
    "**What we got**: All 5 rounds behave exactly as expected! ‚úÖ\n",
    "\n",
    "**How we know it's good**: \n",
    "- Round 1: TFT starts with C, we got R=3 for (C,C) ‚úÖ\n",
    "- Round 2: TFT copied our C, we got T=5 for (D,C) ‚úÖ\n",
    "- Round 3: TFT copied our D, we got P=1 for (D,D) ‚úÖ\n",
    "- Round 4: TFT copied our D, we got S=0 for (C,D) ‚úÖ\n",
    "- Round 5: TFT copied our C, we got R=3 for (C,C) ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üß± BRICK 5: Build the MDP (Markov Decision Process)\n",
    "---\n",
    "\n",
    "## What is this brick doing?\n",
    "\n",
    "Now we get to the MATH! A **Markov Decision Process (MDP)** is a formal way to describe a decision problem. It has:\n",
    "\n",
    "1. **States (S)**: All possible situations the agent can be in\n",
    "2. **Actions (A)**: All possible moves\n",
    "3. **Transition Probabilities P(s'|s,a)**: \"If I'm in state s and take action a, what's the probability I end up in state s'?\"\n",
    "4. **Rewards R(s,a)**: \"If I'm in state s and take action a, how many points do I get on average?\"\n",
    "\n",
    "## Why build this explicitly?\n",
    "\n",
    "Policy Iteration needs to know the EXACT probabilities and rewards. By building the MDP matrices, we can solve for the optimal policy mathematically (no trial-and-error needed!).\n",
    "\n",
    "## Transition Logic:\n",
    "\n",
    "For **deterministic opponents** (ALL_C, ALL_D, TFT):\n",
    "- P(s'|s,a) = 1 for exactly one s', and 0 for all others\n",
    "- We KNOW what the opponent will do\n",
    "\n",
    "For **stochastic opponents** (IMPERFECT_TFT):\n",
    "- P(s'|s,a) = 0.9 for the \"intended\" next state\n",
    "- P(s'|s,a) = 0.1 for the \"slip\" next state\n",
    "- The opponent might mess up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BRICK 5A: GENERATE ALL POSSIBLE STATES\n",
    "# ============================================================\n",
    "\n",
    "def get_states(memory_length):\n",
    "    \"\"\"\n",
    "    Generate all possible states for a given memory length.\n",
    "    \n",
    "    Memory-1: 2^2 = 4 states (my_last, opp_last)\n",
    "    Memory-2: 2^4 = 16 states (my_t-1, my_t-2, opp_t-1, opp_t-2)\n",
    "    \n",
    "    We use itertools.product to generate all combinations of C and D.\n",
    "    \"\"\"\n",
    "    if memory_length == 1:\n",
    "        # All combinations of (my_action, opp_action)\n",
    "        return list(product([COOPERATE, DEFECT], repeat=2))\n",
    "    elif memory_length == 2:\n",
    "        # All combinations of 4 actions\n",
    "        return list(product([COOPERATE, DEFECT], repeat=4))\n",
    "    else:\n",
    "        raise ValueError(\"Only memory 1 or 2 is supported\")\n",
    "\n",
    "\n",
    "def state_to_str(s):\n",
    "    \"\"\"Convert a state tuple to a readable string like (C,D).\"\"\"\n",
    "    return '(' + ','.join(ACTION_NAMES[x] for x in s) + ')'\n",
    "\n",
    "\n",
    "# Display all states\n",
    "print(\"üìä STATE SPACES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nüîπ Memory-1 States (4 total):\")\n",
    "print(\"   Format: (my_last_move, opponent_last_move)\")\n",
    "for s in get_states(1):\n",
    "    meaning = f\"I played {ACTION_NAMES[s[0]]}, opponent played {ACTION_NAMES[s[1]]}\"\n",
    "    print(f\"   {state_to_str(s)} ‚Üí {meaning}\")\n",
    "\n",
    "print(f\"\\nüîπ Memory-2 States: {len(get_states(2))} total\")\n",
    "print(\"   Format: (my_t-1, my_t-2, opp_t-1, opp_t-2)\")\n",
    "print(\"   (Too many to list, but here are a few examples:)\")\n",
    "for s in get_states(2)[:4]:\n",
    "    print(f\"   {state_to_str(s)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BRICK 5B: BUILD TRANSITION AND REWARD MATRICES\n",
    "# ============================================================\n",
    "\n",
    "def build_mdp(opponent_strategy, memory_length):\n",
    "    \"\"\"\n",
    "    Build the complete MDP for a given opponent and memory setting.\n",
    "    \n",
    "    This is the HEART of the mathematical formulation!\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    states : list\n",
    "        All possible states\n",
    "    \n",
    "    P : dict\n",
    "        Transition probabilities: P[s][a][s'] = probability\n",
    "        \"From state s, taking action a, probability of reaching s'\"\n",
    "    \n",
    "    R : dict\n",
    "        Expected rewards: R[s][a] = expected immediate reward\n",
    "        \"From state s, taking action a, average reward\"\n",
    "    \"\"\"\n",
    "    states = get_states(memory_length)\n",
    "    state_set = set(states)  # For fast lookup\n",
    "    \n",
    "    # Initialize empty dictionaries\n",
    "    # P[s][a] will be a dict of {s': probability}\n",
    "    # R[s][a] will be the expected reward\n",
    "    P = {s: {a: {} for a in ACTIONS} for s in states}\n",
    "    R = {s: {a: 0.0 for a in ACTIONS} for s in states}\n",
    "    \n",
    "    # Fill in the matrices\n",
    "    for s in states:\n",
    "        for a in ACTIONS:  # For each action the agent might take\n",
    "            \n",
    "            if memory_length == 1:\n",
    "                # Current state: s = (my_prev, opp_prev)\n",
    "                my_prev = s[0]  # What I did last round\n",
    "                \n",
    "                # What will the opponent do?\n",
    "                # This depends on their strategy and my previous action\n",
    "                \n",
    "                if opponent_strategy == \"ALL_C\":\n",
    "                    # Opponent ALWAYS cooperates\n",
    "                    opp_distribution = {COOPERATE: 1.0}\n",
    "                    \n",
    "                elif opponent_strategy == \"ALL_D\":\n",
    "                    # Opponent ALWAYS defects\n",
    "                    opp_distribution = {DEFECT: 1.0}\n",
    "                    \n",
    "                elif opponent_strategy == \"TFT\":\n",
    "                    # Opponent copies my previous action\n",
    "                    opp_distribution = {my_prev: 1.0}\n",
    "                    \n",
    "                elif opponent_strategy == \"IMPERFECT_TFT\":\n",
    "                    # 90% copy, 10% opposite\n",
    "                    opp_distribution = {\n",
    "                        my_prev: 0.9,\n",
    "                        1 - my_prev: 0.1\n",
    "                    }\n",
    "                \n",
    "                # Now calculate transitions and rewards\n",
    "                for opp_action, prob in opp_distribution.items():\n",
    "                    # Next state: (my_action_now, opp_action_now)\n",
    "                    s_next = (a, opp_action)\n",
    "                    \n",
    "                    # Add to transition probability\n",
    "                    P[s][a][s_next] = P[s][a].get(s_next, 0) + prob\n",
    "                    \n",
    "                    # Add to expected reward\n",
    "                    R[s][a] += prob * PAYOFF_MATRIX[(a, opp_action)]\n",
    "            \n",
    "            elif memory_length == 2:\n",
    "                # Current state: s = (my_t-1, my_t-2, opp_t-1, opp_t-2)\n",
    "                my_t1, my_t2, opp_t1, opp_t2 = s\n",
    "                \n",
    "                # Opponent looks at my_t-1 (my most recent action)\n",
    "                if opponent_strategy == \"ALL_C\":\n",
    "                    opp_distribution = {COOPERATE: 1.0}\n",
    "                elif opponent_strategy == \"ALL_D\":\n",
    "                    opp_distribution = {DEFECT: 1.0}\n",
    "                elif opponent_strategy == \"TFT\":\n",
    "                    opp_distribution = {my_t1: 1.0}\n",
    "                elif opponent_strategy == \"IMPERFECT_TFT\":\n",
    "                    opp_distribution = {my_t1: 0.9, 1 - my_t1: 0.1}\n",
    "                \n",
    "                for opp_action, prob in opp_distribution.items():\n",
    "                    # Next state: shift the history\n",
    "                    # New: (a, my_t1, opp_action, opp_t1)\n",
    "                    s_next = (a, my_t1, opp_action, opp_t1)\n",
    "                    \n",
    "                    P[s][a][s_next] = P[s][a].get(s_next, 0) + prob\n",
    "                    R[s][a] += prob * PAYOFF_MATRIX[(a, opp_action)]\n",
    "    \n",
    "    return states, P, R\n",
    "\n",
    "\n",
    "print(\"‚úÖ MDP builder function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the MDP for Memory-1 against TFT\n",
    "print(\"üìä MDP VISUALIZATION: Memory-1 vs TFT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "states, P, R = build_mdp(\"TFT\", 1)\n",
    "\n",
    "print(\"\\nüéØ TRANSITION PROBABILITIES P(s'|s,a):\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'State':<10} {'Action':<10} {'Next State':<15} {'Probability':<12}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for s in states:\n",
    "    for a in ACTIONS:\n",
    "        for s_next, prob in P[s][a].items():\n",
    "            print(f\"{state_to_str(s):<10} {ACTION_NAMES[a]:<10} {state_to_str(s_next):<15} {prob:.2f}\")\n",
    "\n",
    "print(\"\\nüí∞ EXPECTED REWARDS R(s,a):\")\n",
    "print(\"-\"*40)\n",
    "print(f\"{'State':<10} {'Action C':<15} {'Action D':<15}\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "for s in states:\n",
    "    print(f\"{state_to_str(s):<10} {R[s][COOPERATE]:<15.2f} {R[s][DEFECT]:<15.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now show IMPERFECT_TFT to see the stochastic transitions\n",
    "print(\"\\nüìä MDP VISUALIZATION: Memory-1 vs IMPERFECT_TFT\")\n",
    "print(\"=\"*70)\n",
    "print(\"(Notice: There are now TWO possible next states with 90%/10% split!)\")\n",
    "\n",
    "states, P, R = build_mdp(\"IMPERFECT_TFT\", 1)\n",
    "\n",
    "print(\"\\nüéØ TRANSITION PROBABILITIES P(s'|s,a):\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'State':<10} {'Action':<10} {'Next State':<15} {'Probability':<12}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for s in states:\n",
    "    for a in ACTIONS:\n",
    "        for s_next, prob in sorted(P[s][a].items(), key=lambda x: -x[1]):\n",
    "            print(f\"{state_to_str(s):<10} {ACTION_NAMES[a]:<10} {state_to_str(s_next):<15} {prob:.2f}\")\n",
    "\n",
    "print(\"\\nüí∞ EXPECTED REWARDS R(s,a):\")\n",
    "print(\"(These are now WEIGHTED AVERAGES due to stochastic opponent)\")\n",
    "print(\"-\"*40)\n",
    "print(f\"{'State':<10} {'Action C':<15} {'Action D':<15}\")\n",
    "print(\"-\"*40)\n",
    "\n",
    "for s in states:\n",
    "    print(f\"{state_to_str(s):<10} {R[s][COOPERATE]:<15.2f} {R[s][DEFECT]:<15.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Brick 5 Validation\n",
    "\n",
    "**What we expect**: \n",
    "- TFT: Deterministic transitions (probability = 1.0)\n",
    "- IMPERFECT_TFT: Stochastic transitions (0.9 and 0.1)\n",
    "- Rewards should match payoff matrix (or weighted average)\n",
    "\n",
    "**What we got**: ‚úÖ All correct!\n",
    "\n",
    "**Example verification for TFT**:\n",
    "- State (C,C), Action D ‚Üí Next state (D,C) with prob 1.0 ‚úÖ\n",
    "  - I defect, TFT copies my previous C ‚Üí opponent plays C\n",
    "  - Reward = T = 5 ‚úÖ\n",
    "\n",
    "**Example verification for IMPERFECT_TFT**:\n",
    "- State (C,C), Action C:\n",
    "  - 90% ‚Üí (C,C) with reward 3\n",
    "  - 10% ‚Üí (C,D) with reward 0\n",
    "  - Expected = 0.9√ó3 + 0.1√ó0 = 2.7 ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üß± BRICK 6: Policy Iteration Algorithm\n",
    "---\n",
    "\n",
    "## What is this brick doing?\n",
    "\n",
    "**Policy Iteration** is a classic algorithm to find the OPTIMAL policy (the best action for every state). It works by alternating between two steps:\n",
    "\n",
    "### Step 1: Policy Evaluation\n",
    "\n",
    "Given a policy œÄ (a rule that says \"in state s, do action œÄ(s)\"), calculate the **value** V(s) of each state:\n",
    "\n",
    "$$V^\\pi(s) = R(s, \\pi(s)) + \\gamma \\sum_{s'} P(s'|s, \\pi(s)) V^\\pi(s')$$\n",
    "\n",
    "**In plain English**: \"The value of a state = immediate reward + discounted value of where I end up next\"\n",
    "\n",
    "We solve this by iterating until the values stop changing.\n",
    "\n",
    "### Step 2: Policy Improvement\n",
    "\n",
    "Given the values V(s), update the policy to be GREEDY:\n",
    "\n",
    "$$\\pi'(s) = \\arg\\max_a \\left[ R(s,a) + \\gamma \\sum_{s'} P(s'|s,a) V(s') \\right]$$\n",
    "\n",
    "**In plain English**: \"In each state, pick the action that gives the highest expected value\"\n",
    "\n",
    "### The Loop\n",
    "\n",
    "1. Start with any policy (e.g., always cooperate)\n",
    "2. Evaluate it (calculate V)\n",
    "3. Improve it (make it greedy)\n",
    "4. If the policy didn't change, we're done! Otherwise, go to step 2.\n",
    "\n",
    "## What is Œ≥ (gamma)?\n",
    "\n",
    "The **discount factor** Œ≥ ‚àà (0,1) controls how much we care about the future:\n",
    "\n",
    "- Œ≥ = 0.1 ‚Üí \"I only care about NOW\" (shortsighted)\n",
    "- Œ≥ = 0.99 ‚Üí \"I care a lot about the future\" (farsighted)\n",
    "\n",
    "This is crucial for the Prisoner's Dilemma!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BRICK 6A: POLICY EVALUATION\n",
    "# ============================================================\n",
    "\n",
    "def policy_evaluation(states, P, R, policy, gamma, theta=1e-10):\n",
    "    \"\"\"\n",
    "    Evaluate a policy by computing V^œÄ(s) for all states.\n",
    "    \n",
    "    This solves the Bellman equation iteratively:\n",
    "    V(s) = R(s, œÄ(s)) + Œ≥ Œ£ P(s'|s, œÄ(s)) V(s')\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    states : list\n",
    "        All possible states\n",
    "    P : dict\n",
    "        Transition probabilities P[s][a][s']\n",
    "    R : dict\n",
    "        Rewards R[s][a]\n",
    "    policy : dict\n",
    "        Current policy: policy[s] = action to take in state s\n",
    "    gamma : float\n",
    "        Discount factor (0 < Œ≥ < 1)\n",
    "    theta : float\n",
    "        Convergence threshold (stop when values change less than this)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    V : dict\n",
    "        Value function: V[s] = expected total discounted reward from state s\n",
    "    iterations : int\n",
    "        Number of iterations to converge\n",
    "    \"\"\"\n",
    "    # Initialize all values to 0\n",
    "    V = {s: 0.0 for s in states}\n",
    "    iterations = 0\n",
    "    \n",
    "    while True:\n",
    "        delta = 0  # Track biggest change\n",
    "        \n",
    "        for s in states:\n",
    "            v_old = V[s]\n",
    "            a = policy[s]  # What does the policy say to do?\n",
    "            \n",
    "            # Bellman equation: V(s) = R(s,a) + Œ≥ Œ£ P(s'|s,a) V(s')\n",
    "            v_new = R[s][a]  # Immediate reward\n",
    "            for s_next, prob in P[s][a].items():\n",
    "                v_new += gamma * prob * V[s_next]  # Discounted future\n",
    "            \n",
    "            V[s] = v_new\n",
    "            delta = max(delta, abs(v_old - v_new))\n",
    "        \n",
    "        iterations += 1\n",
    "        \n",
    "        # Stop when values have converged\n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    return V, iterations\n",
    "\n",
    "\n",
    "print(\"‚úÖ Policy Evaluation function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BRICK 6B: POLICY IMPROVEMENT\n",
    "# ============================================================\n",
    "\n",
    "def policy_improvement(states, P, R, V, gamma):\n",
    "    \"\"\"\n",
    "    Improve the policy by acting greedily with respect to V.\n",
    "    \n",
    "    For each state, pick the action that maximizes:\n",
    "    Q(s,a) = R(s,a) + Œ≥ Œ£ P(s'|s,a) V(s')\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    states : list\n",
    "        All possible states\n",
    "    P : dict\n",
    "        Transition probabilities\n",
    "    R : dict\n",
    "        Rewards\n",
    "    V : dict\n",
    "        Current value function\n",
    "    gamma : float\n",
    "        Discount factor\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    new_policy : dict\n",
    "        Improved policy: new_policy[s] = best action for state s\n",
    "    \"\"\"\n",
    "    new_policy = {}\n",
    "    \n",
    "    for s in states:\n",
    "        best_action = None\n",
    "        best_value = float('-inf')\n",
    "        \n",
    "        # Try each action and see which gives highest Q-value\n",
    "        for a in ACTIONS:\n",
    "            # Q(s,a) = R(s,a) + Œ≥ Œ£ P(s'|s,a) V(s')\n",
    "            q_value = R[s][a]\n",
    "            for s_next, prob in P[s][a].items():\n",
    "                q_value += gamma * prob * V[s_next]\n",
    "            \n",
    "            if q_value > best_value:\n",
    "                best_value = q_value\n",
    "                best_action = a\n",
    "        \n",
    "        new_policy[s] = best_action\n",
    "    \n",
    "    return new_policy\n",
    "\n",
    "\n",
    "print(\"‚úÖ Policy Improvement function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BRICK 6C: POLICY ITERATION (MAIN ALGORITHM)\n",
    "# ============================================================\n",
    "\n",
    "def policy_iteration(opponent_strategy, memory_length, gamma, verbose=False):\n",
    "    \"\"\"\n",
    "    Run the complete Policy Iteration algorithm.\n",
    "    \n",
    "    This finds the OPTIMAL policy against a given opponent!\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    opponent_strategy : str\n",
    "        The opponent type\n",
    "    memory_length : int\n",
    "        How many rounds the agent remembers (1 or 2)\n",
    "    gamma : float\n",
    "        Discount factor\n",
    "    verbose : bool\n",
    "        Print progress?\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    policy : dict\n",
    "        Optimal policy\n",
    "    V : dict\n",
    "        Optimal value function\n",
    "    history : list\n",
    "        History of policies during optimization\n",
    "    \"\"\"\n",
    "    # Build the MDP\n",
    "    states, P, R = build_mdp(opponent_strategy, memory_length)\n",
    "    \n",
    "    # Start with \"always cooperate\" policy\n",
    "    policy = {s: COOPERATE for s in states}\n",
    "    \n",
    "    history = []\n",
    "    iteration = 0\n",
    "    \n",
    "    while True:\n",
    "        # Step 1: Policy Evaluation\n",
    "        V, eval_iters = policy_evaluation(states, P, R, policy, gamma)\n",
    "        \n",
    "        # Step 2: Policy Improvement\n",
    "        new_policy = policy_improvement(states, P, R, V, gamma)\n",
    "        \n",
    "        # Check if policy changed\n",
    "        policy_stable = all(policy[s] == new_policy[s] for s in states)\n",
    "        \n",
    "        # Save history\n",
    "        history.append({\n",
    "            'iteration': iteration,\n",
    "            'policy': policy.copy(),\n",
    "            'V': V.copy(),\n",
    "            'eval_iters': eval_iters\n",
    "        })\n",
    "        \n",
    "        if verbose:\n",
    "            policy_str = {state_to_str(s): ACTION_NAMES[a] for s, a in policy.items()}\n",
    "            print(f\"Iteration {iteration}: {policy_str}\")\n",
    "        \n",
    "        # If policy didn't change, we've converged!\n",
    "        if policy_stable:\n",
    "            break\n",
    "        \n",
    "        policy = new_policy\n",
    "        iteration += 1\n",
    "        \n",
    "        # Safety limit\n",
    "        if iteration > 100:\n",
    "            print(\"‚ö†Ô∏è Warning: Max iterations reached\")\n",
    "            break\n",
    "    \n",
    "    return policy, V, history\n",
    "\n",
    "\n",
    "print(\"‚úÖ Policy Iteration algorithm complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test it!\n",
    "print(\"üß™ Testing Policy Iteration\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test against TFT with Œ≥=0.9\n",
    "print(\"\\nüìç Opponent: TFT, Memory: 1, Œ≥ = 0.9\")\n",
    "policy, V, history = policy_iteration(\"TFT\", 1, 0.9, verbose=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Converged in {len(history)} iterations!\")\n",
    "print(f\"\\nOptimal Policy:\")\n",
    "for s in get_states(1):\n",
    "    print(f\"   State {state_to_str(s)} ‚Üí {ACTION_NAMES[policy[s]]}\")\n",
    "\n",
    "print(f\"\\nState Values:\")\n",
    "for s in get_states(1):\n",
    "    print(f\"   V{state_to_str(s)} = {V[s]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Brick 6 Validation\n",
    "\n",
    "**What we expect against TFT with Œ≥=0.9**:\n",
    "- The optimal policy should be to ALWAYS COOPERATE\n",
    "- Why? Because TFT will cooperate back, and 3 per round forever beats short-term exploitation\n",
    "\n",
    "**What we got**: \n",
    "- Policy: Cooperate in ALL states ‚úÖ\n",
    "- Values are positive and similar across states ‚úÖ\n",
    "- Converged quickly ‚úÖ\n",
    "\n",
    "**Why is this optimal?**\n",
    "\n",
    "Let's think about it:\n",
    "- If we ALWAYS cooperate: TFT cooperates back ‚Üí we get 3 every round\n",
    "- Total value = 3 + 0.9√ó3 + 0.81√ó3 + ... = 3/(1-0.9) = 30\n",
    "\n",
    "- If we defect once: We get 5, but then TFT defects back, and we're stuck with 1s\n",
    "- Total value = 5 + 0.9√ó1 + 0.81√ó1 + ... ‚âà 5 + 9 = 14 (much worse!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üß± BRICK 7: Simulation Verification\n",
    "---\n",
    "\n",
    "## What is this brick doing?\n",
    "\n",
    "We've found the optimal policy mathematically. But does it actually WORK in the real environment? Let's simulate:\n",
    "\n",
    "1. Play 50 episodes (games)\n",
    "2. Each episode is 50 steps (rounds)\n",
    "3. Use the optimal policy to choose actions\n",
    "4. Track total rewards\n",
    "\n",
    "The simulated average should match our theoretical predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BRICK 7: SIMULATION VERIFICATION\n",
    "# ============================================================\n",
    "\n",
    "def simulate(opponent_strategy, memory_length, policy, n_episodes=50, n_steps=50, seed=None):\n",
    "    \"\"\"\n",
    "    Simulate the policy playing against the opponent.\n",
    "    \n",
    "    This is the \"real\" test - actually playing the game!\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    mean_reward : float\n",
    "        Average cumulative reward per episode\n",
    "    std_reward : float\n",
    "        Standard deviation\n",
    "    all_rewards : list\n",
    "        Rewards from each episode\n",
    "    episode_data : list\n",
    "        Detailed data from each episode (for visualization)\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    env = PrisonerDilemmaEnv(opponent_strategy, memory_length)\n",
    "    all_rewards = []\n",
    "    episode_data = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        ep_history = []\n",
    "        \n",
    "        for step in range(n_steps):\n",
    "            # Use policy to choose action\n",
    "            action = policy[state]\n",
    "            \n",
    "            # Take step\n",
    "            new_state, reward, _, _, info = env.step(action)\n",
    "            \n",
    "            # Record\n",
    "            ep_history.append({\n",
    "                'step': step,\n",
    "                'state': state,\n",
    "                'action': action,\n",
    "                'opp_action': info['opp_action'],\n",
    "                'reward': reward\n",
    "            })\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = new_state\n",
    "        \n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_data.append(ep_history)\n",
    "    \n",
    "    return np.mean(all_rewards), np.std(all_rewards), all_rewards, episode_data\n",
    "\n",
    "\n",
    "print(\"‚úÖ Simulation function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# RUN ALL EXPERIMENTS\n",
    "# ============================================================\n",
    "\n",
    "OPPONENTS = [\"ALL_C\", \"ALL_D\", \"TFT\", \"IMPERFECT_TFT\"]\n",
    "GAMMAS = [0.1, 0.5, 0.9, 0.99]\n",
    "MEMORIES = [1, 2]\n",
    "\n",
    "# Store all results\n",
    "results = {}\n",
    "\n",
    "print(\"üî¨ RUNNING ALL EXPERIMENTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for memory in MEMORIES:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìä MEMORY-{memory}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    for opponent in OPPONENTS:\n",
    "        print(f\"\\nüéØ Opponent: {opponent}\")\n",
    "        print(\"-\"*50)\n",
    "        \n",
    "        for gamma in GAMMAS:\n",
    "            # Run Policy Iteration\n",
    "            policy, V, history = policy_iteration(opponent, memory, gamma)\n",
    "            \n",
    "            # Simulate\n",
    "            mean_reward, std_reward, all_rewards, ep_data = simulate(\n",
    "                opponent, memory, policy, seed=42\n",
    "            )\n",
    "            \n",
    "            # Count defects in policy\n",
    "            n_defect = sum(1 for a in policy.values() if a == DEFECT)\n",
    "            n_total = len(policy)\n",
    "            \n",
    "            # Store\n",
    "            results[(memory, opponent, gamma)] = {\n",
    "                'policy': policy,\n",
    "                'V': V,\n",
    "                'mean_reward': mean_reward,\n",
    "                'std_reward': std_reward,\n",
    "                'n_defect': n_defect,\n",
    "                'iterations': len(history),\n",
    "                'episode_data': ep_data\n",
    "            }\n",
    "            \n",
    "            # Print summary\n",
    "            policy_summary = \"ALL D\" if n_defect == n_total else (\n",
    "                \"ALL C\" if n_defect == 0 else f\"{n_defect}/{n_total} D\"\n",
    "            )\n",
    "            print(f\"   Œ≥={gamma:<4}: {policy_summary:<10} | Reward: {mean_reward:>6.2f} ¬± {std_reward:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ All experiments complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üß± BRICK 8: Visualization with Animation\n",
    "---\n",
    "\n",
    "## What is this brick doing?\n",
    "\n",
    "Let's create a visual animation of the game being played! We'll show:\n",
    "\n",
    "1. Each round's moves (icons for C and D)\n",
    "2. The rewards earned\n",
    "3. The running total score\n",
    "4. Export as a GIF!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BRICK 8: GAME VISUALIZATION\n",
    "# ============================================================\n",
    "\n",
    "def create_game_frames(episode_data, opponent_name, max_frames=20):\n",
    "    \"\"\"\n",
    "    Create frames for the game animation.\n",
    "    \n",
    "    Returns a list of PIL Image objects.\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    \n",
    "    # Use a subset of steps for the animation\n",
    "    steps_to_show = min(len(episode_data), max_frames)\n",
    "    \n",
    "    cumulative_reward = 0\n",
    "    \n",
    "    for i in range(steps_to_show):\n",
    "        step_data = episode_data[i]\n",
    "        cumulative_reward += step_data['reward']\n",
    "        \n",
    "        # Create figure\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        ax.set_xlim(0, 10)\n",
    "        ax.set_ylim(0, 6)\n",
    "        ax.set_aspect('equal')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Title\n",
    "        ax.text(5, 5.5, f\"Round {i+1} vs {opponent_name}\", \n",
    "                ha='center', va='center', fontsize=20, fontweight='bold')\n",
    "        \n",
    "        # Agent side\n",
    "        agent_color = '#4CAF50' if step_data['action'] == COOPERATE else '#F44336'\n",
    "        agent_text = 'ü§ù' if step_data['action'] == COOPERATE else 'üó°Ô∏è'\n",
    "        ax.add_patch(patches.FancyBboxPatch((1, 2), 3, 2.5, \n",
    "                     boxstyle=\"round,pad=0.1\", facecolor=agent_color, alpha=0.3))\n",
    "        ax.text(2.5, 4, \"AGENT\", ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "        ax.text(2.5, 3, agent_text, ha='center', va='center', fontsize=40)\n",
    "        ax.text(2.5, 2.3, ACTION_NAMES[step_data['action']], \n",
    "                ha='center', va='center', fontsize=16)\n",
    "        \n",
    "        # Opponent side\n",
    "        opp_color = '#4CAF50' if step_data['opp_action'] == COOPERATE else '#F44336'\n",
    "        opp_text = 'ü§ù' if step_data['opp_action'] == COOPERATE else 'üó°Ô∏è'\n",
    "        ax.add_patch(patches.FancyBboxPatch((6, 2), 3, 2.5,\n",
    "                     boxstyle=\"round,pad=0.1\", facecolor=opp_color, alpha=0.3))\n",
    "        ax.text(7.5, 4, \"OPPONENT\", ha='center', va='center', fontsize=14, fontweight='bold')\n",
    "        ax.text(7.5, 3, opp_text, ha='center', va='center', fontsize=40)\n",
    "        ax.text(7.5, 2.3, ACTION_NAMES[step_data['opp_action']], \n",
    "                ha='center', va='center', fontsize=16)\n",
    "        \n",
    "        # Reward\n",
    "        ax.text(5, 1.2, f\"Reward: +{step_data['reward']}\", \n",
    "                ha='center', va='center', fontsize=18, color='blue')\n",
    "        ax.text(5, 0.5, f\"Total: {cumulative_reward}\", \n",
    "                ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Convert to image\n",
    "        fig.canvas.draw()\n",
    "        img = Image.frombytes('RGB', fig.canvas.get_width_height(),\n",
    "                             fig.canvas.tostring_rgb())\n",
    "        frames.append(img)\n",
    "        plt.close(fig)\n",
    "    \n",
    "    return frames\n",
    "\n",
    "\n",
    "def save_animation(frames, filename, duration=500):\n",
    "    \"\"\"\n",
    "    Save frames as a GIF.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    frames : list of PIL Images\n",
    "    filename : str\n",
    "    duration : int\n",
    "        Milliseconds per frame\n",
    "    \"\"\"\n",
    "    if len(frames) > 0:\n",
    "        frames[0].save(\n",
    "            filename,\n",
    "            save_all=True,\n",
    "            append_images=frames[1:],\n",
    "            duration=duration,\n",
    "            loop=0\n",
    "        )\n",
    "        print(f\"‚úÖ Animation saved to {filename}\")\n",
    "    else:\n",
    "        print(\"‚ùå No frames to save\")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Animation functions created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create animations for interesting scenarios\n",
    "print(\"üé¨ Creating Game Animations...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Animation 1: Cooperation with TFT (Œ≥=0.9)\n",
    "print(\"\\nüìπ Animation 1: Optimal play vs TFT\")\n",
    "ep_data = results[(1, \"TFT\", 0.9)]['episode_data'][0]\n",
    "frames = create_game_frames(ep_data, \"TFT\", max_frames=15)\n",
    "save_animation(frames, \"game_tft_coop.gif\")\n",
    "\n",
    "# Animation 2: Defection against ALL_C\n",
    "print(\"\\nüìπ Animation 2: Optimal play vs ALL_C (exploitation!)\")\n",
    "ep_data = results[(1, \"ALL_C\", 0.9)]['episode_data'][0]\n",
    "frames = create_game_frames(ep_data, \"ALL_C\", max_frames=15)\n",
    "save_animation(frames, \"game_allc_exploit.gif\")\n",
    "\n",
    "# Animation 3: Mutual defection with ALL_D\n",
    "print(\"\\nüìπ Animation 3: Optimal play vs ALL_D (mutual defection)\")\n",
    "ep_data = results[(1, \"ALL_D\", 0.9)]['episode_data'][0]\n",
    "frames = create_game_frames(ep_data, \"ALL_D\", max_frames=15)\n",
    "save_animation(frames, \"game_alld_defect.gif\")\n",
    "\n",
    "# Animation 4: Noisy cooperation with IMPERFECT_TFT\n",
    "print(\"\\nüìπ Animation 4: Optimal play vs IMPERFECT_TFT (forgiving!)\")\n",
    "ep_data = results[(1, \"IMPERFECT_TFT\", 0.9)]['episode_data'][0]\n",
    "frames = create_game_frames(ep_data, \"IMPERFECT_TFT\", max_frames=20)\n",
    "save_animation(frames, \"game_imperfect_tft.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the animations (in Colab/Jupyter)\n",
    "from IPython.display import Image as IPImage, display\n",
    "\n",
    "print(\"üé¨ Displaying Animations:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ Cooperation vs TFT (Everyone wins!)\")\n",
    "display(IPImage(filename=\"game_tft_coop.gif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n2Ô∏è‚É£ Exploitation vs ALL_C (Agent takes advantage)\")\n",
    "display(IPImage(filename=\"game_allc_exploit.gif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n3Ô∏è‚É£ Mutual Defection vs ALL_D (Nobody wins)\")\n",
    "display(IPImage(filename=\"game_alld_defect.gif\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n4Ô∏è‚É£ Forgiveness vs IMPERFECT_TFT (Noise doesn't break cooperation!)\")\n",
    "display(IPImage(filename=\"game_imperfect_tft.gif\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üìä Results Analysis and Key Findings\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# COMPREHENSIVE RESULTS TABLE\n",
    "# ============================================================\n",
    "\n",
    "print(\"üìä COMPLETE RESULTS TABLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create a nice formatted table\n",
    "for memory in [1, 2]:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"MEMORY-{memory}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{'Opponent':<15} {'Œ≥':<6} {'Policy':<15} {'Avg Reward':<15} {'Expected':<15}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for opponent in OPPONENTS:\n",
    "        for gamma in GAMMAS:\n",
    "            r = results[(memory, opponent, gamma)]\n",
    "            \n",
    "            # Policy summary\n",
    "            n_def = r['n_defect']\n",
    "            n_tot = len(r['policy'])\n",
    "            if n_def == n_tot:\n",
    "                policy_str = \"ALL DEFECT\"\n",
    "            elif n_def == 0:\n",
    "                policy_str = \"ALL COOPERATE\"\n",
    "            else:\n",
    "                policy_str = f\"{n_def}/{n_tot} Defect\"\n",
    "            \n",
    "            # Calculate expected reward per step\n",
    "            expected_per_step = r['mean_reward'] / 50\n",
    "            \n",
    "            print(f\"{opponent:<15} {gamma:<6} {policy_str:<15} {r['mean_reward']:<15.2f} {expected_per_step:.2f}/step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZATION: POLICY HEATMAP\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "for idx, opponent in enumerate(OPPONENTS):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Create heatmap data\n",
    "    data = []\n",
    "    for gamma in GAMMAS:\n",
    "        r = results[(1, opponent, gamma)]\n",
    "        defect_ratio = r['n_defect'] / len(r['policy'])\n",
    "        data.append(defect_ratio)\n",
    "    \n",
    "    # Plot\n",
    "    colors = ['#4CAF50' if d == 0 else '#F44336' if d == 1 else '#FFC107' for d in data]\n",
    "    bars = ax.bar(range(len(GAMMAS)), data, color=colors)\n",
    "    \n",
    "    ax.set_xticks(range(len(GAMMAS)))\n",
    "    ax.set_xticklabels([f'Œ≥={g}' for g in GAMMAS])\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.set_ylabel('Fraction of States ‚Üí Defect')\n",
    "    ax.set_title(opponent, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Add labels\n",
    "    for bar, d in zip(bars, data):\n",
    "        label = 'ALL C' if d == 0 else 'ALL D' if d == 1 else 'Mixed'\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,\n",
    "                label, ha='center', fontsize=9)\n",
    "\n",
    "plt.suptitle('Optimal Policy by Opponent and Discount Factor (Memory-1)', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('policy_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Interpretation:\")\n",
    "print(\"   ‚Ä¢ Green (ALL C): Agent always cooperates\")\n",
    "print(\"   ‚Ä¢ Red (ALL D): Agent always defects\")\n",
    "print(\"   ‚Ä¢ Yellow (Mixed): Agent's action depends on the state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# VISUALIZATION: REWARD COMPARISON\n",
    "# ============================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, memory in enumerate([1, 2]):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    for opponent in OPPONENTS:\n",
    "        rewards = [results[(memory, opponent, g)]['mean_reward'] for g in GAMMAS]\n",
    "        ax.plot(GAMMAS, rewards, 'o-', label=opponent, linewidth=2, markersize=8)\n",
    "    \n",
    "    ax.set_xlabel('Discount Factor (Œ≥)', fontsize=12)\n",
    "    ax.set_ylabel('Average Cumulative Reward (50 steps)', fontsize=12)\n",
    "    ax.set_title(f'Memory-{memory}', fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='best')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_ylim(0, 260)\n",
    "\n",
    "plt.suptitle('Reward vs Discount Factor by Opponent Type', \n",
    "             fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('reward_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Key Observations:\")\n",
    "print(\"   ‚Ä¢ Against ALL_C: Always get max reward (250) by exploiting\")\n",
    "print(\"   ‚Ä¢ Against ALL_D: Always get min reward (50) - can't cooperate\")\n",
    "print(\"   ‚Ä¢ Against TFT: Reward increases with Œ≥ as cooperation emerges\")\n",
    "print(\"   ‚Ä¢ Against IMPERFECT_TFT: Similar to TFT but slightly lower due to noise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FIND CRITICAL GAMMA FOR COOPERATION\n",
    "# ============================================================\n",
    "\n",
    "print(\"üîç Finding Critical Œ≥ for Cooperation (Memory-1 vs TFT)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "test_gammas = np.arange(0.05, 1.0, 0.05)\n",
    "defect_fractions = []\n",
    "\n",
    "for gamma in test_gammas:\n",
    "    policy, _, _ = policy_iteration(\"TFT\", 1, gamma)\n",
    "    n_defect = sum(1 for a in policy.values() if a == DEFECT)\n",
    "    frac = n_defect / len(policy)\n",
    "    defect_fractions.append(frac)\n",
    "    \n",
    "    status = \"ALL D\" if frac == 1 else \"ALL C\" if frac == 0 else \"MIXED\"\n",
    "    print(f\"Œ≥ = {gamma:.2f}: {status}\")\n",
    "\n",
    "# Find transition point\n",
    "for i in range(len(defect_fractions)-1):\n",
    "    if defect_fractions[i] > 0 and defect_fractions[i+1] == 0:\n",
    "        print(f\"\\n‚ú® CRITICAL THRESHOLD: Œ≥ ‚âà {test_gammas[i+1]:.2f}\")\n",
    "        print(f\"   Below {test_gammas[i+1]:.2f}: Agent defects (shortsighted)\")\n",
    "        print(f\"   Above {test_gammas[i+1]:.2f}: Agent cooperates (farsighted)\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üìù Executive Summary\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë                              EXECUTIVE SUMMARY                                    ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë                                                                                  ‚ïë\n",
    "‚ïë  üéØ QUESTION 1: Effect of Discount Factor (Œ≥)                                   ‚ïë\n",
    "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                   ‚ïë\n",
    "‚ïë  ‚Ä¢ Low Œ≥ (0.1-0.5): Agent is shortsighted, always DEFECTS                       ‚ïë\n",
    "‚ïë    - Prefers immediate T=5 over future rewards                                   ‚ïë\n",
    "‚ïë    - Gets stuck in mutual defection with TFT (1 point/round)                    ‚ïë\n",
    "‚ïë                                                                                  ‚ïë\n",
    "‚ïë  ‚Ä¢ High Œ≥ (0.9-0.99): Agent is farsighted, COOPERATES with TFT                  ‚ïë\n",
    "‚ïë    - Values long-term relationship                                               ‚ïë\n",
    "‚ïë    - Maintains mutual cooperation (3 points/round)                               ‚ïë\n",
    "‚ïë                                                                                  ‚ïë\n",
    "‚ïë  ‚Ä¢ Critical threshold: Œ≥ ‚âà 0.65-0.70 against TFT                                ‚ïë\n",
    "‚ïë                                                                                  ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë                                                                                  ‚ïë\n",
    "‚ïë  üß† QUESTION 2: Memory Depth (Memory-1 vs Memory-2)                             ‚ïë\n",
    "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                             ‚ïë\n",
    "‚ïë  ‚Ä¢ Finding: Memory-2 provides NO advantage against these 4 opponents            ‚ïë\n",
    "‚ïë                                                                                  ‚ïë\n",
    "‚ïë  ‚Ä¢ Why? These opponents only use 1-step history:                                ‚ïë\n",
    "‚ïë    - ALL_C: Ignores history completely                                          ‚ïë\n",
    "‚ïë    - ALL_D: Ignores history completely                                          ‚ïë\n",
    "‚ïë    - TFT: Only looks at t-1                                                      ‚ïë\n",
    "‚ïë    - IMPERFECT_TFT: Only looks at t-1                                           ‚ïë\n",
    "‚ïë                                                                                  ‚ïë\n",
    "‚ïë  ‚Ä¢ Counter-example: Tit-for-Two-Tats (TF2T)                                     ‚ïë\n",
    "‚ïë    - Punishes only after TWO consecutive defections                              ‚ïë\n",
    "‚ïë    - Memory-2 agent can see both t-1 and t-2                                    ‚ïë\n",
    "‚ïë    - Optimal: Defect, Cooperate, Defect, Cooperate... (exploit without punish) ‚ïë\n",
    "‚ïë    - Memory-1 cannot distinguish D-D from C-D, so cannot exploit                ‚ïë\n",
    "‚ïë                                                                                  ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë                                                                                  ‚ïë\n",
    "‚ïë  üé≤ QUESTION 3: Noise Analysis (TFT vs IMPERFECT_TFT)                           ‚ïë\n",
    "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                           ‚ïë\n",
    "‚ïë  ‚Ä¢ Does 10% noise break cooperation? NO!                                        ‚ïë\n",
    "‚ïë                                                                                  ‚ïë\n",
    "‚ïë  ‚Ä¢ At high Œ≥, optimal policy is still ALL COOPERATE                             ‚ïë\n",
    "‚ïë    - Agent learns to be \"forgiving\"                                             ‚ïë\n",
    "‚ïë    - Absorbs occasional exploitation to maintain relationship                    ‚ïë\n",
    "‚ïë    - Expected reward drops ~10% (135 vs 150) due to random slips                ‚ïë\n",
    "‚ïë                                                                                  ‚ïë\n",
    "‚ïë  ‚Ä¢ Why doesn't the agent retaliate?                                             ‚ïë\n",
    "‚ïë    - If agent defects back, triggers defection spiral                           ‚ïë\n",
    "‚ïë    - Long-term cost > short-term benefit                                        ‚ïë\n",
    "‚ïë    - Forgiveness is mathematically optimal!                                      ‚ïë\n",
    "‚ïë                                                                                  ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë                                                                                  ‚ïë\n",
    "‚ïë  üí° KEY INSIGHT:                                                                 ‚ïë\n",
    "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                                                ‚ïë\n",
    "‚ïë  Policy Iteration mathematically proves that COOPERATION is RATIONAL            ‚ïë\n",
    "‚ïë  against responsive opponents (TFT) when agents value the future.               ‚ïë\n",
    "‚ïë                                                                                  ‚ïë\n",
    "‚ïë  This validates Axelrod's famous finding: \"Nice\" strategies that cooperate     ‚ïë\n",
    "‚ïë  first and retaliate/forgive appropriately tend to win in the long run.        ‚ïë\n",
    "‚ïë                                                                                  ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ‚úÖ Validation Summary: What We Expected vs What We Got\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "‚ïë                       VALIDATION: EXPECTED vs ACTUAL                              ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë                                                                                  ‚ïë\n",
    "‚ïë  üìã TEST 1: Against ALL_C (Always Cooperating Opponent)                         ‚ïë\n",
    "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                           ‚ïë\n",
    "‚ïë  Expected: Always defect (exploit the nice guy)                                 ‚ïë\n",
    "‚ïë  Got:      ALL DEFECT for all Œ≥ values                              ‚úÖ PASS     ‚ïë\n",
    "‚ïë  Reward:   250 (50 steps √ó 5 points = max possible)                 ‚úÖ CORRECT  ‚ïë\n",
    "‚ïë                                                                                  ‚ïë\n",
    "‚ïë  üìã TEST 2: Against ALL_D (Always Defecting Opponent)                           ‚ïë\n",
    "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                           ‚ïë\n",
    "‚ïë  Expected: Always defect (no point cooperating)                                 ‚ïë\n",
    "‚ïë  Got:      ALL DEFECT for all Œ≥ values                              ‚úÖ PASS     ‚ïë\n",
    "‚ïë  Reward:   50 (50 steps √ó 1 point = min when opponent defects)      ‚úÖ CORRECT  ‚ïë\n",
    "‚ïë                                                                                  ‚ïë\n",
    "‚ïë  üìã TEST 3: Against TFT with low Œ≥ (0.1)                                        ‚ïë\n",
    "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                           ‚ïë\n",
    "‚ïë  Expected: Defect (shortsighted, want immediate T=5)                            ‚ïë\n",
    "‚ïë  Got:      ALL DEFECT                                               ‚úÖ PASS     ‚ïë\n",
    "‚ïë  Reward:   ~54 (initial 5, then stuck at 1)                         ‚úÖ CORRECT  ‚ïë\n",
    "‚ïë                                                                                  ‚ïë\n",
    "‚ïë  üìã TEST 4: Against TFT with high Œ≥ (0.9)                                       ‚ïë\n",
    "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                           ‚ïë\n",
    "‚ïë  Expected: Cooperate (value future 3s over immediate 5)                         ‚ïë\n",
    "‚ïë  Got:      ALL COOPERATE                                            ‚úÖ PASS     ‚ïë\n",
    "‚ïë  Reward:   150 (50 steps √ó 3 points)                                ‚úÖ CORRECT  ‚ïë\n",
    "‚ïë                                                                                  ‚ïë\n",
    "‚ïë  üìã TEST 5: Against IMPERFECT_TFT with high Œ≥ (0.9)                             ‚ïë\n",
    "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                           ‚ïë\n",
    "‚ïë  Expected: Still cooperate (forgive the 10% mistakes)                           ‚ïë\n",
    "‚ïë  Got:      ALL COOPERATE                                            ‚úÖ PASS     ‚ïë\n",
    "‚ïë  Reward:   ~135 (less than 150 due to random opponent slips)        ‚úÖ CORRECT  ‚ïë\n",
    "‚ïë                                                                                  ‚ïë\n",
    "‚ïë  üìã TEST 6: Memory-1 vs Memory-2 (same opponents)                               ‚ïë\n",
    "‚ïë  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                           ‚ïë\n",
    "‚ïë  Expected: Same policies and rewards (opponents don't use t-2)                  ‚ïë\n",
    "‚ïë  Got:      Identical results                                        ‚úÖ PASS     ‚ïë\n",
    "‚ïë                                                                                  ‚ïë\n",
    "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
    "‚ïë                                                                                  ‚ïë\n",
    "‚ïë  üéâ ALL TESTS PASSED! The implementation is correct.                            ‚ïë\n",
    "‚ïë                                                                                  ‚ïë\n",
    "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# üèÅ End of Assignment\n",
    "---\n",
    "\n",
    "This notebook has covered:\n",
    "\n",
    "1. ‚úÖ **Part I**: Built the Gymnasium environment with multiple opponent strategies\n",
    "2. ‚úÖ **Part II**: Defined the MDP with explicit transition and reward matrices\n",
    "3. ‚úÖ **Part III**: Implemented Policy Iteration from scratch\n",
    "4. ‚úÖ **Part IV**: Ran experiments and analyzed results\n",
    "\n",
    "Key deliverables:\n",
    "- Transition tables for all scenarios\n",
    "- Optimal policies for all (opponent √ó Œ≥ √ó memory) combinations\n",
    "- Simulation verification (50 episodes √ó 50 steps)\n",
    "- Visualizations and animations\n",
    "- Answers to all analysis questions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
